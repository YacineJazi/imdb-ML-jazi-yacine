{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:19:24.700393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-29 11:19:24.700469: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from glob import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from azureml.core import Dataset, Datastore, Experiment, Run, Workspace\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Flatten, Input, concatenate, Dense, Activation, Dropout, BatchNormalization,  MaxPooling2D, AveragePooling2D, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from azureml.core import Run\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from dotnetcore2 import runtime\n",
    "runtime.version = (\"18\", \"10\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #cli_auth = AzureCliAuthentication()\n",
    "\n",
    "    #Connect to workspace\n",
    "    ws = Workspace.from_config('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=\"imdb_train\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"imdb_training\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='Test data folder mounting point')\n",
    "parser.add_argument('--epochs', type=str, dest='epochs', help='Amount of epochs to train')\n",
    "parser.add_argument('--batch_size', type=str, dest='batch_size', help='Batch size')\n",
    "parser.add_argument('--model_name', type=str, dest='model_name', help='Model name')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "dataset_train = np.load(os.path.join(data_folder, 'dataset_train.npy'))\n",
    "dataset_test = np.load(os.path.join(data_folder, 'dataset_test.npy'))\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "batch_size = int(args.batch_size)\n",
    "epochs = int(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the autoencoder\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "autoencoder = Sequential()\n",
    "#Decode\n",
    "autoencoder.add(InputLayer((3952,)))\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "#Bottleneck\n",
    "autoencoder.add(Dense(120, activation= 'relu' ))\n",
    "#Encode\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "\n",
    "autoencoder.add(Dense(3952, activation= 'sigmoid' ))\n",
    "autoencoder.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the autoencoder with a custom loss function\n",
    "\n",
    "def custom_loss(y_true,y_pred):\n",
    "    y_mask=tf.keras.backend.clip(y_true, 0, 0.01)*100\n",
    "    return K.mean(K.square(y_mask*(y_pred - y_true)), axis=-1)\n",
    "\n",
    "early_stopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "autoencoder.compile(loss=custom_loss, optimizer='adam')\n",
    "autoencoder.fit(np.array(dataset_train),\n",
    "                np.array(dataset_train),\n",
    "                validation_split=0.2,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                callbacks=[reduce_lr,early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_null = []\n",
    "pred_not_null = []\n",
    "predictions = autoencoder.predict(dataset_test)\n",
    "for i in range(len(dataset_train)):\n",
    "    indeces = np.nonzero(np.array(dataset_train.iloc[i-1]))\n",
    "    test = []\n",
    "    pred = []\n",
    "    test_not_null.append(np.array([dataset_train.iloc[i-1,index] for index in indeces[0]], dtype=float))\n",
    "    pred_not_null.append(np.array([predictions[i-1][index] for index in indeces[0]], dtype=float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "means = []\n",
    "for i in range(len(test_not_null)):\n",
    "    means.append(np.mean(np.power(np.array(test_not_null[i]) - np.array(pred_not_null)[i],2)))\n",
    "mse = np.mean(means)\n",
    "print(\"MSE\")\n",
    "print(np.mean(mse))\n",
    "print(\"STD\")\n",
    "variances=[]\n",
    "for i in range(len(test_not_null)):\n",
    "    n = len(test_not_null[i])\n",
    "    mean = sum((np.array(test_not_null[i]))) / n\n",
    "    deviations = [(x - mean) ** 2 for x in np.array(np.array(test_not_null[i]))]\n",
    "    variance = sum(deviations) / n\n",
    "    variances.append(variance)\n",
    "    \n",
    "std = math.sqrt(np.mean(variances))\n",
    "print(std)\n",
    "#Super Low std => Values don't lay far from apart => the review scores don't lay far apart"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07521bee647cb038a9765ea0ecd3eab4d12b5f5b9eec9493f2d4acdd5b019b2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
