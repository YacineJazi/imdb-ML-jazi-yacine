{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:19:24.700393: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-29 11:19:24.700469: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from glob import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from azureml.core import Dataset, Datastore, Experiment, Run, Workspace\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Flatten, Input, concatenate, Dense, Activation, Dropout, BatchNormalization,  MaxPooling2D, AveragePooling2D, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from azureml.core import Run\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from dotnetcore2 import runtime\n",
    "runtime.version = (\"18\", \"10\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #cli_auth = AzureCliAuthentication()\n",
    "\n",
    "    #Connect to workspace\n",
    "    ws = Workspace.from_config('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=\"imdb_train\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"imdb_training\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: data/tmp/train_test_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/tmp/train_test_data/dataset_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1310911/726182808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data folder:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/tmp/train_test_data/dataset_train.npy'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='Test data folder mounting point')\n",
    "parser.add_argument('--epochs', type=str, dest='epochs', help='Amount of epochs to train')\n",
    "parser.add_argument('--batch_size', type=str, dest='batch_size', help='Batch size')\n",
    "parser.add_argument('--model_name', type=str, dest='model_name', help='Model name')\n",
    "args = parser.parse_args(['--data-folder','data/tmp/train_test_data','--epochs','20','--batch_size','5','--model_name','imdb-trained'])\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "dataset_train = np.load(os.path.join(data_folder, 'dataset_train.npy'))\n",
    "dataset_test = np.load(os.path.join(data_folder, 'dataset_test.npy'))\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "batch_size = int(args.batch_size)\n",
    "epochs = int(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the autoencoder\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "autoencoder = Sequential()\n",
    "#Decode\n",
    "autoencoder.add(InputLayer((3952,)))\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "#Bottleneck\n",
    "autoencoder.add(Dense(120, activation= 'relu' ))\n",
    "#Encode\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "\n",
    "autoencoder.add(Dense(3952, activation= 'sigmoid' ))\n",
    "autoencoder.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the autoencoder with a custom loss function\n",
    "\n",
    "def custom_loss(y_true,y_pred):\n",
    "    y_mask=tf.keras.backend.clip(y_true, 0, 0.01)*100\n",
    "    return K.mean(K.square(y_mask*(y_pred - y_true)), axis=-1)\n",
    "\n",
    "early_stopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "autoencoder.compile(loss=custom_loss, optimizer='adam')\n",
    "autoencoder.fit(np.array(dataset_train),\n",
    "                np.array(dataset_train),\n",
    "                validation_split=0.2,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                callbacks=[reduce_lr,early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_null = []\n",
    "pred_not_null = []\n",
    "predictions = autoencoder.predict(dataset_test)\n",
    "for i in range(len(dataset_train)):\n",
    "    indeces = np.nonzero(np.array(dataset_train.iloc[i-1]))\n",
    "    test = []\n",
    "    pred = []\n",
    "    test_not_null.append(np.array([dataset_train.iloc[i-1,index] for index in indeces[0]], dtype=float))\n",
    "    pred_not_null.append(np.array([predictions[i-1][index] for index in indeces[0]], dtype=float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "means = []\n",
    "for i in range(len(test_not_null)):\n",
    "    means.append(np.mean(np.power(np.array(test_not_null[i]) - np.array(pred_not_null)[i],2)))\n",
    "mse = np.mean(means)\n",
    "print(\"MSE\")\n",
    "print(np.mean(mse))\n",
    "print(\"STD\")\n",
    "variances=[]\n",
    "for i in range(len(test_not_null)):\n",
    "    n = len(test_not_null[i])\n",
    "    mean = sum((np.array(test_not_null[i]))) / n\n",
    "    deviations = [(x - mean) ** 2 for x in np.array(np.array(test_not_null[i]))]\n",
    "    variance = sum(deviations) / n\n",
    "    variances.append(variance)\n",
    "    \n",
    "std = math.sqrt(np.mean(variances))\n",
    "print(std)\n",
    "#Super Low std => Values don't lay far from apart => the review scores don't lay far apart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('imdb-training-env')\n",
    "cd = CondaDependencies.create(\n",
    "    pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'scikit-learn'],\n",
    "    )\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"imdb-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "model_name = \"tveer-model\"\n",
    "args = ['--data-folder', train_test_dataset.as_mount(), '--epochs', 30, '--batch_size', 32, '--model_name', model_name]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07521bee647cb038a9765ea0ecd3eab4d12b5f5b9eec9493f2d4acdd5b019b2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
