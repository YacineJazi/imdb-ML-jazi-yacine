{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 19:52:52.239888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-29 19:52:52.240047: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from glob import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from azureml.core import Dataset, Datastore, Experiment, Run, Workspace\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Flatten, Input, concatenate, Dense, Activation, Dropout, BatchNormalization,  MaxPooling2D, AveragePooling2D, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from azureml.core import Run\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from dotnetcore2 import runtime\n",
    "runtime.version = (\"18\", \"10\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Connect to workspace\n",
    "    ws = Workspace.from_config('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp = Experiment(workspace=ws, name=\"imdb_train\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"imdb_training\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: data/tmp/train_test_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='Test data folder mounting point')\n",
    "parser.add_argument('--epochs', type=str, dest='epochs', help='Amount of epochs to train')\n",
    "parser.add_argument('--batch_size', type=str, dest='batch_size', help='Batch size')\n",
    "parser.add_argument('--model_name', type=str, dest='model_name', help='Model name')\n",
    "args = parser.parse_args(['--data-folder','data/tmp/train_test_data','--epochs','20','--batch_size','5','--model_name','imdb-trained'])\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "dataset_train = np.load(os.path.join(data_folder, 'dataset_train.npy'))\n",
    "dataset_test = np.load(os.path.join(data_folder, 'dataset_test.npy'))\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "batch_size = int(args.batch_size)\n",
    "epochs = int(args.epochs)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the autoencoder\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "autoencoder = Sequential()\n",
    "#Decode\n",
    "autoencoder.add(InputLayer((3952,)))\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "#Bottleneck\n",
    "autoencoder.add(Dense(120, activation= 'relu' ))\n",
    "#Encode\n",
    "autoencoder.add(Dense(1000, activation= 'relu' ))\n",
    "\n",
    "autoencoder.add(Dense(3952, activation= 'sigmoid' ))\n",
    "autoencoder.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the autoencoder with a custom loss function\n",
    "\n",
    "def custom_loss(y_true,y_pred):\n",
    "    y_mask=tf.keras.backend.clip(y_true, 0, 0.01)*100\n",
    "    return K.mean(K.square(y_mask*(y_pred - y_true)), axis=-1)\n",
    "\n",
    "early_stopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "autoencoder.compile(loss=custom_loss, optimizer='adam')\n",
    "autoencoder.fit(np.array(dataset_train),\n",
    "                np.array(dataset_train),\n",
    "                validation_split=0.2,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                callbacks=[reduce_lr,early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_null = []\n",
    "pred_not_null = []\n",
    "predictions = autoencoder.predict(dataset_test)\n",
    "for i in range(len(dataset_train)):\n",
    "    indeces = np.nonzero(np.array(dataset_train.iloc[i-1]))\n",
    "    test = []\n",
    "    pred = []\n",
    "    test_not_null.append(np.array([dataset_train.iloc[i-1,index] for index in indeces[0]], dtype=float))\n",
    "    pred_not_null.append(np.array([predictions[i-1][index] for index in indeces[0]], dtype=float))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "means = []\n",
    "for i in range(len(test_not_null)):\n",
    "    means.append(np.mean(np.power(np.array(test_not_null[i]) - np.array(pred_not_null)[i],2)))\n",
    "mse = np.mean(means)\n",
    "print(\"MSE\")\n",
    "print(np.mean(mse))\n",
    "print(\"STD\")\n",
    "variances=[]\n",
    "for i in range(len(test_not_null)):\n",
    "    n = len(test_not_null[i])\n",
    "    mean = sum((np.array(test_not_null[i]))) / n\n",
    "    deviations = [(x - mean) ** 2 for x in np.array(np.array(test_not_null[i]))]\n",
    "    variance = sum(deviations) / n\n",
    "    variances.append(variance)\n",
    "    \n",
    "std = math.sqrt(np.mean(variances))\n",
    "print(std)\n",
    "#Super Low std => Values don't lay far from apart => the review scores don't lay far apart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"imdb-training-env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-dataset-runtime[pandas,fuse]\",\n",
       "                        \"azureml-defaults\",\n",
       "                        \"tensorflow\",\n",
       "                        \"scikit-learn\",\n",
       "                        \"pandas\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"azureml_db79fe6ea2f89b12c6768a4257c1f3b7\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"2\"\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('imdb-training-env')\n",
    "cd = CondaDependencies.create(\n",
    "    pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults', 'tensorflow', 'scikit-learn','pandas'],\n",
    "    )\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target: imdb-cluster\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-11-29T18:58:06.140000+00:00', 'errors': None, 'creationTime': '2021-11-29T15:25:24.425870+00:00', 'modifiedTime': '2021-11-29T16:24:49.261775+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"imdb-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes,\n",
    "                                                                identity_type=\"SystemAssigned\")\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "model_name = \"imdb_model\"\n",
    "train_test_dataset = Dataset.get_by_name(ws, name='imdb_train_test')\n",
    "args = ['--data-folder', train_test_dataset.as_mount(), '--epochs', epochs, '--batch_size', batch_size, '--model_name', model_name]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=\"../steps/root/scripts\",\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>imdb_train</td><td>imdb_train_1638215255_2591be2b</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/imdb_train_1638215255_2591be2b?wsid=/subscriptions/350ff211-0872-40d5-a629-9b672edbe452/resourcegroups/imdb-jazi-yacine/workspaces/imdbML-jazi-yacine&amp;tid=4ded4bb1-6bff-42b3-aed7-6a36a503bf7a\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: imdb_train,\n",
       "Id: imdb_train_1638215255_2591be2b,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'states/training-run.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1774477/1428629668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrun_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'inputDatasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputDatasets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'states/training-run.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'states/training-run.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_run_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_run_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'states/training-run.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "run.wait_for_completion()\n",
    "\n",
    "run_details = {k:v for k,v in run.get_details().items() if k not in ['inputDatasets', 'outputDatasets']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"states/training-run.json\"\n",
    "\n",
    "if not os.path.exists(os.path.dirname(filename)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        raise\n",
    "\n",
    "\n",
    "with open('filename', 'w') as training_run_json:\n",
    "    json.dump(run_details, training_run_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb_model\timdb_model:1\t1\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model(model_name=\"imdb_model\", model_path=f'outputs/{model_name}')\n",
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "model_json = {}\n",
    "model_json[\"model\"] = model.serialize()\n",
    "model_json[\"run\"] = run_details\n",
    "\n",
    "with open('states/model_details.json', 'w') as model_details:\n",
    "    json.dump(model_json, model_details)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07521bee647cb038a9765ea0ecd3eab4d12b5f5b9eec9493f2d4acdd5b019b2b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
